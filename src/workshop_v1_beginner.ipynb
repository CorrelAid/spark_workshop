{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following presents a few simple tasks designed for initial orientation and to introduce some basic PySpark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a Spark Session, please execute this cell always first!\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Pyspark Intro Taks\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Read the dataset _GroceryDataset.csv_ from the data directory and check the schema. How many different datatypes appear in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 1</summary>\n",
    "<code>\n",
    "df = spark.read.csv(\"../data/GroceryDataset.csv\", header=True, inferSchema=True)<br>\n",
    "# Get datatypes - Option 1:\n",
    "df.printSchema()\n",
    "<br>\n",
    "# Get datatypes - Option 2:\n",
    "distinct_dataTypes = df.dtypes\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Display the first 5 and the last 5 rows of the dataset. What might be the reason that a _PySparkValueError_ occurs when you try to show the last 5 rows?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 2</summary>\n",
    "<code>\n",
    "#Display the first 5 rows:\n",
    "spark.createDataFrame(df.head(5)).show()\n",
    "<br>\n",
    "#Display the last 5 rows:\n",
    "spark.createDataFrame(df.tail(5)).show()\n",
    "# This throws an PySparkValueError because the last 17 rows of column \"Rating\" have the value 'NULL'. Therefore spark cannot determine the Value type of this column without further information.\n",
    "# If you want to display the last rows of the table you have to set n=18 instead of n=5.\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Count the total (distinct) number of records. Then, drop all rows containing _NULL_ values. Now, count the total (distinct) number of records again. <br> What is the problem if you simply drop all rows containing _NULL_ values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 3</summary>\n",
    "<code>\n",
    "total_number = df.distinct().count()\n",
    "print(total_number)\n",
    "total_number_noNULL = df.dropna().distinct().count()\n",
    "print(total_number_noNULL)\n",
    "<br>#Problem: The dataset is significantly smaller after dropping all the rows containing 'NULL' values. This can directly impact all further data analyzation and machine learning training as important rows might be dropped during the process.\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Clean Dataset/Data Preparation - Part 1: Address missing values (_NULL_-Values)**\n",
    "* In the column _Currency_ replace all _NULL_ with the given Currency symbol.\n",
    "* In the column _Discount_ replace all _NULL_ with _No Discount_ instead.\n",
    "* In the column _Title_ replace all _NULL_ with _No Title_.\n",
    "* In the column _Feature_ replace all _NULL_ with _No Feature_.\n",
    "* In the column _Product Description_ replace all _NULL_ with _No Description_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 4 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 4</summary>\n",
    "<code>\n",
    "df = df.withColumn('Currency', F.when(F.col('Currency').isNull(),'$').otherwise(F.col('Currency'))) \\\n",
    ".withColumn('Discount', F.when(F.col('Discount').isNull(),'No Discount').otherwise(F.col('Discount'))) \\\n",
    ".withColumn('Title',F.when(F.col('Title').isNull(),'No Title').otherwise(F.col('Title'))) \\\n",
    ".withColumn('Feature',F.when(F.col('Feature').isNull(),'No Feature').otherwise(F.col('Feature'))) \\\n",
    ".withColumn('Product Description',F.when(F.col('Product Description').isNull(),'No Product Description').otherwise(F.col('Product Description'))) \n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Clean Dataset/Data Preparation - Part 2:<br> In the column _Rating_ extract the rating that is written after \"Rated\". Use the regular expression \"\\d+\\.\\d+\" to extract only numbers like \"4.3\". Additionally make sure only 0 appears otherwise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 5 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 5</summary>\n",
    "<code>\n",
    "df = df.withColumn('Rating', F.regexp_extract(F.col('Rating'), \"\\d+(\\.\\d+)?\", 0)) \\\n",
    ".withColumn('Rating', \n",
    "            F.when(F.col('Rating').isNull(), 0)\n",
    "            .when(F.col('Rating')==\"\",0)\n",
    "            .otherwise(F.col('Rating')))\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Clean Dataset/Data Preparation - Part 3:<br>Remove all string values in the column _Pricev. Additionally remove the $ -symbol and cast the price to numerical type. Then calculate the mean of the column _Pricev, round the mean value to two decimals and replace all the _NULL_ in the column with the calculated mean.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 6</summary>\n",
    "<code>\n",
    "df =df.withColumn('Price',F.when(F.col('Price').startswith(\"$\"), F.substring_index(F.col('Price'), \"$\", -1)).otherwise(0)) \\\n",
    ".withColumn('Price', F.col('Price').cast('float'))\n",
    "<br>\n",
    "#Calculate the mean and replace all 0 values with the mean\n",
    "price_mean_value = round(df.agg(F.mean(F.col('Price'))).collect()[0][0],2)\n",
    "df = df.withColumn('Price', F.when(F.col(\"Price\") == 0.0,price_mean_value).otherwise(F.col('Price')))\\\n",
    ".withColumn('Price', F.round(F.col('Price')))\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Filter the dataset to include only entries where the _Rating_ is greater than 0 and less than or equal to 5, and then arrange them in ascending order. How many rows do have a rating?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 7 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 7</summary>\n",
    "<code>\n",
    "df_ratings = df.filter((F.col('Rating') > 0) & (F.col('Rating') <= 5))\n",
    "ordered_df_ratings=df_ratings.orderBy(F.col('Rating'),asc=True)\n",
    "total_ratings = ordered_df_ratings.distinct().count()\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Group the dataset by the categorical column _Sub Category_ and calculate the average of ratings and prices and take the first occuring product id. <br>Rename the aggregated columns to meaningful names. Which Discount results in the best rating? Which Discount results in the worst rating?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 8 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 8</summary>\n",
    "<code>\n",
    "# Aggregate dataframe - Option 1:<br>\n",
    "df_grouped = df_ratings.groupBy('Sub Category').agg(F.avg('Rating'), F.avg('Price'), F.first('Product_ID'))\n",
    "<br>\n",
    "# Aggregate dataframe - Option 2:<br>\n",
    "df_grouped = df_ratings.groupBy('Sub Category').agg({\"Rating\": \"avg\",\"Price\": \"avg\",\"Product_ID\": \"first\"})\n",
    "<br>\n",
    "#Rename the Columns:\n",
    "df_grouped = df_grouped.withColumnRenamed('first(Product_ID)','First_Product_ID')\\\n",
    ".withColumnRenamed('avg(Rating)','Average_Rating')\\\n",
    ".withColumnRenamed('avg(Price)','Average_Price')\n",
    "<br>\n",
    "# Order DataFrame by column 'Discount' - Option 1:\n",
    "df_grouped = df_grouped.orderBy('Average_Rating', asc=False)\n",
    "<br>\n",
    "# Order DataFrame by column 'Discount' - Option 2:\n",
    "df_grouped.sort(F.desc('Average_Rating'))\n",
    "\n",
    "Best Rating: Breakfast\n",
    "Worst Rating: Non-GMO\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Left Join the grouped dataset of task 8 with the cleaned dataset of task 6 based on the ID and only select the ID, the categeory, the average rating, the normal rating, the average price and the normal price. Order the joined DataFrame by the ascending ID.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 9 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 9</summary>\n",
    "<code>\n",
    "df_joined = df_grouped.join(df, on=df_grouped.First_Product_ID==df.Product_ID, how='left').select('First_Product_ID','Product_ID','Average_Rating','Rating','Average_Price', 'Price')\n",
    "df_joined = df_joined.sort(F.asc('Product_ID'))\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Write and save the processed dataset to an output file \"GroceryDataset_solution\" as a .parquet-file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 10 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 10</summary>\n",
    "<code>\n",
    "df_joined.write.mode(\"overwrite\").parquet(\"../data/GroceryDataset_solution.parquet\")\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this setup is solely for training purposes. In reality, the dataset is unevenly weighted and not suitable for tasks like Machine Learning. This training is intended to provide an initial understanding of how to use PySpark and perform basic data cleaning steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
