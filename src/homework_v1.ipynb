{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following presents a few simple tasks designed for initial orientation and to introduce some basic PySpark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Pyspark Intro Taks\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the dataset dataset1 from the data directory and check the schema. How many different datatypes appear in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 1</summary>\n",
    "<code>\n",
    "df = spark.read.csv(\"../data/GroceryDataset.csv\", header=True, inferSchema=True)<br>\n",
    "# Get datatypes - Option 1:\n",
    "df.printSchema()\n",
    "<br>\n",
    "# Get datatypes - Option 2:\n",
    "distinct_dataTypes = df.dtypes\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display the first 5 and the last 5 rows of the dataset. What might be the reason that a PySparkValueError occurs when you try to show the last 5 rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 2</summary>\n",
    "<code>\n",
    "#Display the first 5 rows:\n",
    "spark.createDataFrame(df.head(5)).show()\n",
    "<br>\n",
    "#Display the last 5 rows:\n",
    "spark.createDataFrame(df.tail(5)).show()\n",
    "# This throws an PySparkValueError because the last 17 rows of column \"Rating\" have the value 'NULL'. Therefore spark cannot determine the Value type of this column without further information.\n",
    "# If you want to display the last rows of the table you have to set n=18 instead of n=5.\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Count the total (distinct) number of records. Then, drop all rows containing 'NULL' values. Now, count the total (distinct) number of records again. <br> What is the problem if you simply drop all rows containing 'NULL' values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 3</summary>\n",
    "<code>\n",
    "total_number = df.distinct().count()\n",
    "print(total_number)\n",
    "total_number_noNULL = df.dropna().distinct().count()\n",
    "print(total_number_noNULL)\n",
    "<br>#The dataset is significantly smaller after dropping all the rows containing 'NULL' values. This can directly impact all further data analyzation and machine learning training as important rows might be dropped during the process.\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Clean Dataset/Data Preparation - Part 1: Address missing values (Null-Values)\n",
    "    * In the column 'Currency' replace all 'NULL' with the given Currency symbol.\n",
    "    * In the column 'Discount' replace all 'NULL' with 'No Discount' instead.\n",
    "    * In the column 'Title' replace all 'NULL' with 'No Title'.\n",
    "    * In the column 'Feature' replace all 'NULL' with 'No Feature'.\n",
    "    * In the column 'Product Description' replace all 'NULL' with 'No Description'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 4 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 4</summary>\n",
    "<code>\n",
    "df = df.withColumn('Currency', F.when(F.col('Currency').isNull(),'$').otherwise(F.col('Currency'))) \\\n",
    ".withColumn('Discount', F.when(F.col('Discount').isNull(),'No Discount').otherwise(F.col('Discount'))) \\\n",
    ".withColumn('Title',F.when(F.col('Title').isNull(),'No Title').otherwise(F.col('Title'))) \\\n",
    ".withColumn('Feature',F.when(F.col('Feature').isNull(),'No Feature').otherwise(F.col('Feature'))) \\\n",
    ".withColumn('Product Description',F.when(F.col('Product Description').isNull(),'No Product Description').otherwise(F.col('Product Description'))) \\\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Clean Dataset/Data Preparation - Part 2:<br> In the column 'Rating' extract the rating that is written after \"Rated\". Use the regular expression \"\\d+(\\.\\d+)?\" to extract only the numbers. Additionally make sure only 0 appears otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 5 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 5</summary>\n",
    "<code>\n",
    "df = df.withColumn('Rating', F.regexp_extract(F.col('Rating'), \"\\d+(\\.\\d+)?\", 0)) \\\n",
    ".withColumn('Rating', \n",
    "            F.when(F.col('Rating').isNull(), 0)\n",
    "            .when(F.col('Rating')==\"\",0)\n",
    "            .otherwise(F.col('Rating')))\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Clean Dataset/Data Preparation - Part 3:<br>Remove all string values in the column 'Price'. Additionally remove the $ -symbol and cast the price to numerical type. Then calculate the mean of the column 'Price', round the mean value to two decimals and replace all the 'NULL' in the column with the calculated mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 6</summary>\n",
    "<code>\n",
    "df =df.withColumn('Price',F.when(F.col('Price').startswith(\"$\"), F.substring_index(F.col('Price'), \"$\", -1)).otherwise(0)) \\\n",
    ".withColumn('Price', F.col('Price').cast('float'))\n",
    "<br>\n",
    "#Calculate the mean and replace all 0 values with the mean\n",
    "price_mean_value = round(df.agg(F.mean(F.col('Price'))).collect()[0][0],2)\n",
    "df = df.withColumn('Price', F.when(F.col(\"Price\") == 0.0,price_mean_value).otherwise(F.col('Price')))\\\n",
    ".withColumn('Price', F.round(F.col('Price')))\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Filter the dataset where the condition X is fullfiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 7 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 7</summary>\n",
    "<code>\n",
    "df = df.filter()\n",
    "<br>\n",
    "<br>\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Aggregate the dataset: Calculate the sum and the mean of the column x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 8 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 8</summary>\n",
    "<code>\n",
    "# Aggregate dataframe - Option 1:<br>\n",
    "agg_df = df.agg(\n",
    "    F.sum(F.col()),\n",
    "    F.mean(F.col())\n",
    ")\n",
    "<br>\n",
    "<br>\n",
    "# Aggregate dataframe - Option 2:<br>\n",
    "agg_df = df.agg(\n",
    "    {\"numeric_column\": \"sum\",\n",
    "     \"another_numeric_column\": \"mean\"}\n",
    "     )\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Group the dataset by the categorical column x and calculate the sum of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 9 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 9</summary>\n",
    "<code>\n",
    "<br>\n",
    "<br>\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Left Join the dataset with dataset2 based on a common key which you have to identify yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 10 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 10</summary>\n",
    "<code>\n",
    "<br>\n",
    "<br>\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Write and save the processed dataset to an output file \"GroceryDataset_solution\" as a .parquet-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 11 - Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution - 11</summary>\n",
    "<code>\n",
    "df.write.mode(\"overwrite\").parquet(\"../data/GroceryDataset_solution.parquet\")\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this setup is solely for training purposes. In reality, the dataset is unevenly weighted and not suitable for tasks like Machine Learning. This training is intended to provide an initial understanding of how to use PySpark and perform basic data cleaning steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
